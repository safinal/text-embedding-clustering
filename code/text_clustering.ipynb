{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-bvG77Tc5U7C"
   },
   "source": [
    "# **Text Clustering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5ga-6hdYMpu"
   },
   "source": [
    "## **Imports** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "o9nNkrZE5U7I"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import umap.umap_ as umap\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import tensorflow as tf\n",
    "import keras.backend as K\n",
    "\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from keras.layers import Dense, Input\n",
    "from keras.models import Model\n",
    "from keras.initializers import VarianceScaling, glorot_uniform\n",
    "\n",
    "# random_state: mutual_info_classif, KMeans, MiniBatchKMeans, SpectralClustering, SpectralBiclustering, SpectralCoclustering, NMF, silhouette_score, VarianceScaling, glorot_uniform\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, TfidfTransformer, CountVectorizer\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_classif \n",
    "from sklearn.cluster import KMeans, MiniBatchKMeans, SpectralClustering, SpectralBiclustering, SpectralCoclustering, AgglomerativeClustering, Birch\n",
    "from sklearn.decomposition import NMF\n",
    "from sklearn.metrics import normalized_mutual_info_score, adjusted_rand_score, homogeneity_score, adjusted_mutual_info_score, silhouette_score, confusion_matrix\n",
    "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
    "\n",
    "from scipy import sparse\n",
    "from scipy.optimize import linear_sum_assignment as linear_assignment\n",
    "\n",
    "from IPython.display import Image\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "tf.random.set_seed(10)\n",
    "np.random.seed(10)\n",
    "random_state = np.random.RandomState(seed=10)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-5ga-6hdYMpu"
   },
   "source": [
    "## **Metrics** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "aAGuIzwW5inR"
   },
   "outputs": [],
   "source": [
    "nmi_fun = normalized_mutual_info_score\n",
    "ari_fun = adjusted_rand_score\n",
    "ami_fun = adjusted_mutual_info_score\n",
    "silhouette_score_fun = silhouette_score\n",
    "\n",
    "# acc_fun = homogeneity_score\n",
    "\n",
    "def acc_fun (y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate clustering accuracy. Require scikit-learn installed\n",
    "\n",
    "    # Arguments\n",
    "        y: true labels, numpy.array with shape `(n_samples,)`\n",
    "        y_pred: predicted labels, numpy.array with shape `(n_samples,)`\n",
    "\n",
    "    # Return\n",
    "        accuracy, in [0,1]\n",
    "    \"\"\"\n",
    "    y_true = y_true.astype(np.int64)\n",
    "    assert y_pred.size == y_true.size\n",
    "    D = max(y_pred.max(), y_true.max()) + 1\n",
    "    w = np.zeros((D, D), dtype=np.int64)\n",
    "    for i in range(y_pred.size):\n",
    "        w[y_pred[i], y_true[i]] += 1\n",
    "    ind = list(linear_assignment(w.max() - w))\n",
    "    acc_sum = 0\n",
    "    for k in range(len(w)):\n",
    "       acc_sum =  w[ind[0][k],ind[1][k]] + acc_sum\n",
    "    return acc_sum * 1.0 / y_pred.size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmPIYGwYYeum"
   },
   "source": [
    "## Dataset Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### WEBKB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bQER6yb3P83C",
    "outputId": "6f6b7fb2-9762-4db0-f1cf-7de2a5192df5"
   },
   "outputs": [],
   "source": [
    "def fetching_data_set():\n",
    "    data = []\n",
    "    f = open(\"../data/webkb/webkb-all-stemmed.txt\", \"r\")\n",
    "    for x in f:\n",
    "        data.append(x)\n",
    "    f.close()\n",
    "    print('Number of documents: ', len(data))\n",
    "    return data\n",
    "\n",
    "data = fetching_data_set()\n",
    "raw_data = []\n",
    "target = []\n",
    "for i in data:\n",
    "  ff = i.split(\"\\t\")\n",
    "  target.append(ff[0])\n",
    "  raw_data.append(ff[1])\n",
    "\n",
    "max_features = 2000\n",
    "\n",
    "# x = CountVectorizer(dtype=np.float64, max_features=max_features).fit_transform(raw_data)\n",
    "x = CountVectorizer(dtype=np.float64).fit_transform(raw_data)\n",
    "x = TfidfTransformer(norm='l2', sublinear_tf=True).fit_transform(x)\n",
    "x = np.asarray(x.todense()) * np.sqrt(x.shape[1])\n",
    "\n",
    "print('todense succeed')\n",
    "\n",
    "y = []\n",
    "for i in target:\n",
    "  if i == \"student\":\n",
    "    y.append(0)\n",
    "  elif i == \"faculty\":\n",
    "    y.append(1)\n",
    "  elif i == \"project\":\n",
    "    y.append(2)\n",
    "  elif i == \"course\":\n",
    "    y.append(3)\n",
    "\n",
    "y = np.asarray(y)\n",
    "\n",
    "# p = random_state.permutation(x.shape[0])\n",
    "# x, y = x[p], y[p]\n",
    "# print('permutation finished')\n",
    "n_clusters = 4\n",
    "\n",
    "# for i in range(x.shape[0]):\n",
    "#   x[i] = (x[i]-np.min(x[i]))/(np.max(x[i])-np.min(x[i]))\n",
    "main_inputs = x\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reutersidf10k"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def load_reuters(max_features, data_path='../data/reuters'):\n",
    "#     print('making reuters idf features')\n",
    "#     make_reuters_data(data_path, max_features)\n",
    "#     print('reutersidf saved to ' + data_path)\n",
    "#     data = np.load(os.path.join(data_path, 'reutersidf10k.npy'), allow_pickle=True).item()\n",
    "#     # has been shuffled\n",
    "#     x = data['data']\n",
    "#     y = data['label']\n",
    "#     x = x.reshape((x.shape[0], -1)).astype('float32')\n",
    "#     y = y.reshape((y.size,))\n",
    "#     #print('REUTERSIDF10K samples', x.shape)\n",
    "#     return x, y\n",
    "\n",
    "\n",
    "# def make_reuters_data(data_dir, max_features):\n",
    "#     did_to_cat = {}\n",
    "#     cat_list = ['CCAT', 'GCAT', 'MCAT', 'ECAT']\n",
    "#     with open(os.path.join(data_dir, 'rcv1-v2.topics.qrels')) as fin:\n",
    "#         for line in fin.readlines():\n",
    "#             line = line.strip().split(' ')\n",
    "#             cat = line[0]\n",
    "#             did = int(line[1])\n",
    "#             if cat in cat_list:\n",
    "#                 did_to_cat[did] = did_to_cat.get(did, []) + [cat]\n",
    "#         for did in list(did_to_cat.keys()):\n",
    "#             if len(did_to_cat[did]) > 1:\n",
    "#                 del did_to_cat[did]\n",
    "\n",
    "#     dat_list = ['lyrl2004_tokens_test_pt0.dat',\n",
    "#                 'lyrl2004_tokens_test_pt1.dat',\n",
    "#                 'lyrl2004_tokens_test_pt2.dat',\n",
    "#                 'lyrl2004_tokens_test_pt3.dat',\n",
    "#                 'lyrl2004_tokens_train.dat']\n",
    "#     data = []\n",
    "#     target = []\n",
    "#     cat_to_cid = {'CCAT': 0, 'GCAT': 1, 'MCAT': 2, 'ECAT': 3}\n",
    "#     del did\n",
    "#     for dat in dat_list:\n",
    "#         with open(os.path.join(data_dir, dat)) as fin:\n",
    "#             for line in fin.readlines():\n",
    "#                 if line.startswith('.I'):\n",
    "#                     if 'did' in locals():\n",
    "#                         assert doc != ''\n",
    "#                         if did in did_to_cat:\n",
    "#                             data.append(doc)\n",
    "#                             target.append(cat_to_cid[did_to_cat[did][0]])\n",
    "#                     did = int(line.strip().split(' ')[1])\n",
    "#                     doc = ''\n",
    "#                 elif line.startswith('.W'):\n",
    "#                     assert doc == ''\n",
    "#                 else:\n",
    "#                     doc += line\n",
    "\n",
    "#     assert len(data) == len(did_to_cat)\n",
    "\n",
    "#     x = CountVectorizer(dtype=np.float64, max_features=max_features).fit_transform(data)\n",
    "#     # x = CountVectorizer(dtype=np.float64).fit_transform(data)\n",
    "#     print(x.shape)\n",
    "#     y = np.asarray(target)\n",
    "\n",
    "#     x = TfidfTransformer(norm='l2', sublinear_tf=True).fit_transform(x)\n",
    "#     x, y = x[:10000], y[:10000]\n",
    "#     x = np.asarray(x.todense()) * np.sqrt(x.shape[1])\n",
    "#     print('todense succeed')\n",
    "\n",
    "#     p = random_state.permutation(x.shape[0])\n",
    "#     x, y = x[p], y[p]\n",
    "#     print('permutation finished')\n",
    "\n",
    "#     assert x.shape[0] == y.shape[0]\n",
    "#     x = x.reshape((x.shape[0], -1))\n",
    "#     print(x.shape)\n",
    "#     np.save(os.path.join(data_dir, 'reutersidf10k.npy'), {'data': x, 'label': y})\n",
    "\n",
    "# max_features = 500\n",
    "# x,y = load_reuters(max_features=max_features)\n",
    "# n_clusters = 2\n",
    "\n",
    "# for i in range(x.shape[0]):\n",
    "#     x[i] = (x[i]-np.min(x[i]))/(np.max(x[i])-np.min(x[i]))\n",
    "# main_inputs = x\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# newsgroups = fetch_20newsgroups(subset='all')\n",
    "# y = np.asarray(newsgroups.target)\n",
    "\n",
    "# max_features = 2000\n",
    "\n",
    "# x = CountVectorizer(dtype=np.float64, max_features=max_features).fit_transform(newsgroups.data)\n",
    "# # x = CountVectorizer(dtype=np.float64).fit_transform(newsgroups.data)\n",
    "# x = TfidfTransformer(norm='l2', sublinear_tf=True).fit_transform(x)\n",
    "# x = np.asarray(x.todense()) * np.sqrt(x.shape[1])\n",
    "# print('todense succeed')\n",
    "\n",
    "# p = random_state.permutation(x.shape[0])\n",
    "# x, y = x[p], y[p]\n",
    "# print('permutation finished')\n",
    "# n_clusters = 2\n",
    "# main_inputs = x\n",
    "# x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1H8uXrxy1Vsb"
   },
   "source": [
    "## Laplacian Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1ujcThrWC0hm"
   },
   "outputs": [],
   "source": [
    "#measure distance between vectors v and w\n",
    "def calculate_hamming_distance(x):\n",
    "    hamming_distance_matrix = np.zeros((len(x),len(x)))\n",
    "    for i in tqdm(range(0, len(x))):\n",
    "        for j in range(0, len(x)):\n",
    "            if(hamming_distance_matrix[i][j]!=0 or hamming_distance_matrix[j][i]!=0):\n",
    "                continue\n",
    "            v = x[i]\n",
    "            w = x[j]\n",
    "\n",
    "            v_sum = np.sum(v)\n",
    "            w_sum = np.sum(w)\n",
    "\n",
    "            temp = 1-np.sign(v*w)\n",
    "            v_numerator = np.sum(v*temp)\n",
    "            w_numerator = np.sum(w*temp)\n",
    "\n",
    "            hamming_distance_matrix[i][j] = (v_numerator/v_sum) + (w_numerator/w_sum)\n",
    "            hamming_distance_matrix[j][i] = (v_numerator/v_sum) + (w_numerator/w_sum)\n",
    "    \n",
    "    return hamming_distance_matrix\n",
    "\n",
    "w = cosine_similarity(x)\n",
    "print(w,\"\\n-----\",w.shape)\n",
    "\n",
    "hamming_distance = calculate_hamming_distance(x)\n",
    "print(hamming_distance,\"\\n-----\",hamming_distance.shape)\n",
    "\n",
    "w = w / (np.square(hamming_distance) + w)\n",
    "print(w,\"\\n-----\",w.shape)\n",
    "\n",
    "for i in range(len(w)):\n",
    "    w[i,i] = 0\n",
    "\n",
    "for i in range(len(w)):\n",
    "    s_w = sorted(w[i])[-10:]\n",
    "    for j in range(len(w[i])):\n",
    "        if w[i,j] not in s_w:\n",
    "            w[i,j] = 0\n",
    "\n",
    "D = np.zeros(w.shape)\n",
    "for i in range(len(w)):\n",
    "    D[i, i] = np.count_nonzero(w[i])\n",
    "\n",
    "laplacian_matrix = np.zeros(w.shape)\n",
    "\n",
    "for i in range(len(laplacian_matrix)):\n",
    "    for j in range(len(laplacian_matrix)):\n",
    "        if i == j and D[i, j] != 0:\n",
    "            laplacian_matrix[i, j] = 1\n",
    "        elif i != j and w[i, j] != 0:\n",
    "            laplacian_matrix[i, j] = (-1)/np.sqrt(D[i, i] * D[j, j])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Other Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CB-txohGuXN_",
    "outputId": "a789f7fa-100a-4ee9-a396-813d438ea405",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def pre_processing(data):\n",
    "    porter = PorterStemmer()\n",
    "    vectorizer = TfidfVectorizer(stop_words='english')\n",
    "\n",
    "    # Aggregating and vectorizing all texts\n",
    "    all_text = [\" \".join(data)]\n",
    "    X = vectorizer.fit_transform(all_text)\n",
    "    print('Dimension of vectorizing aggregated texts without stemming: ', X.shape)\n",
    "\n",
    "    # # Opening dictionary\n",
    "    # f = open('words.txt', 'r')\n",
    "    # dictionary = f.read().split()\n",
    "    # f.close()\n",
    "\n",
    "    # # Filtering terms via dictionary and stemming\n",
    "    # token_words = terms\n",
    "    # print('All terms: ', len(token_words))\n",
    "    # # stem_sentence = [porter.stem(word) for word in token_words if word in dictionary]\n",
    "    # stem_sentence = [porter.stem(word) for word in token_words]\n",
    "    #\n",
    "    # all_text_stem = [\" \".join(stem_sentence)]\n",
    "    # print('Number of terms after filtering dictionary: ', len(stem_sentence))\n",
    "    #\n",
    "    # # Vectorizing aggregated texts after filtering and stemming\n",
    "    # X = vectorizer.fit_transform(all_text_stem)\n",
    "    # print('Dimension of vectorizing after filtering and stemming: ', X.shape)\n",
    "\n",
    "    # Stemming texts separately and vectorizing\n",
    "    texts_stem = []\n",
    "    target = []\n",
    "    for i in range(len(data)):\n",
    "        token_words = word_tokenize(data[i])\n",
    "        stem_sentence = []\n",
    "        target.append(\"\".join(token_words[0]))\n",
    "        for word in token_words[1:]:\n",
    "            stem_sentence.append(porter.stem(word.lower()))\n",
    "        texts_stem.append(\" \".join(stem_sentence))\n",
    "\n",
    "    X = vectorizer.fit_transform(texts_stem)\n",
    "    print('Dimension of tf-idf matrix ', X.shape)\n",
    "\n",
    "    return X, target\n",
    "\n",
    "\n",
    "def feature_selection(X, features_number):\n",
    "    # Feature selection using mutual information\n",
    "    X_new = SelectKBest(mutual_info_classif, k=features_number, random_state=10).fit_transform(X, target)\n",
    "    print('Dimension of tf-idf matrix after mutual information: ', X_new.shape)\n",
    "    return X_new\n",
    "\n",
    "\n",
    "def removing_zero_rows(X_new, target):\n",
    "    # Removing zero rows\n",
    "    target_new = target\n",
    "    X_new2 = X_new.todense()\n",
    "    l = np.where(X_new2.sum(axis=1) != 0)[0]\n",
    "    X_new = X_new2[l, :]\n",
    "    index = []\n",
    "    c = 0\n",
    "    j = 0\n",
    "    for i in range(X_new2.shape[0]):\n",
    "        if X_new2[i].sum() == 0:\n",
    "            target_new = np.delete(target_new, j)\n",
    "            index.append(j)\n",
    "            j = j - 1\n",
    "            c = c + 1\n",
    "        j = j + 1\n",
    "    X_new = sparse.csr_matrix(X_new)\n",
    "    print('The number of removed documents: ', c, X_new.shape)\n",
    "    return X_new, target_new\n",
    "\n",
    "\n",
    "def compared_methods(clusters, X_new, target_new):\n",
    "    # Clustering\n",
    "    kmeans = KMeans(n_clusters=clusters, max_iter=200, random_state=10).fit(X_new)\n",
    "    print('KMeans NMI: ', normalized_mutual_info_score(kmeans.labels_, target_new))\n",
    "    print('KMeans AMI: ', adjusted_mutual_info_score(kmeans.labels_, target_new))\n",
    "    print('KMeans Silhouette: ', silhouette_score(X_new, kmeans.labels_, random_state=10))\n",
    "\n",
    "    model = NMF(n_components=clusters, init='random', random_state=10)\n",
    "    D = model.fit_transform(X_new)\n",
    "    W = model.components_\n",
    "    NMF_labels = np.argmax(D, axis=1)\n",
    "    print('NMF NMI: ', normalized_mutual_info_score(NMF_labels, target_new))\n",
    "    print('NMF AMI: ', adjusted_mutual_info_score(NMF_labels, target_new))\n",
    "    print('NMF Silhouette: ', silhouette_score(X_new, NMF_labels, random_state=10))\n",
    "\n",
    "    model = NMF(n_components=clusters, init='random', solver='mu', beta_loss='kullback-leibler', random_state=10)\n",
    "    D = model.fit_transform(X_new)\n",
    "    W = model.components_\n",
    "    NMF_KL_labels = np.argmax(D, axis=1)\n",
    "    print('NMF_KL NMI: ', normalized_mutual_info_score(NMF_KL_labels, target_new))\n",
    "    print('NMF_KL AMI: ', adjusted_mutual_info_score(NMF_KL_labels, target_new))\n",
    "    print('NMF_KL Silhouette: ', silhouette_score(X_new, NMF_KL_labels, random_state=10))\n",
    "\n",
    "    # Spectral = SpectralClustering(n_clusters=clusters, random_state=10).fit(X_new)\n",
    "    # print('Spectral NMI: ', normalized_mutual_info_score(Spectral.labels_, target_new))\n",
    "    # print('Spectral AMI: ', adjusted_mutual_info_score(Spectral.labels_, target_new))\n",
    "    # print('Spectral Silhouette: ', silhouette_score(X_new, Spectral.labels_, random_state=10))\n",
    "\n",
    "    # SpectralBi = SpectralBiclustering(n_clusters=clusters, random_state=10).fit(X_new)\n",
    "    # print('SpectralBi NMI: ', normalized_mutual_info_score(SpectralBi.row_labels_, target_new))\n",
    "    # print('SpectralBi AMI: ', adjusted_mutual_info_score(SpectralBi.row_labels_, target_new))\n",
    "    # print('SpectralBi Silhouette: ', silhouette_score(X_new, SpectralBi.row_labels_, random_state=10))\n",
    "\n",
    "    # SpectralCo = SpectralCoclustering(n_clusters=clusters, random_state=10).fit(X_new)\n",
    "    # print('SpectralCo NMI: ', normalized_mutual_info_score(SpectralCo.row_labels_, target_new))\n",
    "    # print('SpectralCo AMI: ', adjusted_mutual_info_score(SpectralCo.row_labels_, target_new))\n",
    "    # print('SpectralCo Silhouette: ', silhouette_score(X_new, SpectralCo.row_labels_, random_state=10))\n",
    "\n",
    "    Agglomerative = AgglomerativeClustering(n_clusters=clusters).fit(X_new)\n",
    "    print('Agglomerative NMI: ', normalized_mutual_info_score(Agglomerative.labels_, target_new))\n",
    "    print('Agglomerative AMI: ', adjusted_mutual_info_score(Agglomerative.labels_, target_new))\n",
    "    print('Agglomerative Silhouette: ', silhouette_score(X_new, Agglomerative.labels_, random_state=10))\n",
    "\n",
    "    brc = Birch(n_clusters=clusters).fit(X_new)\n",
    "    labels = brc.predict(X_new)\n",
    "    print('Birch NMI: ', normalized_mutual_info_score(labels, target_new))\n",
    "    print('Birch AMI: ', adjusted_mutual_info_score(labels, target_new))\n",
    "    print('Birch Silhouette: ', silhouette_score(X_new, labels, random_state=10))\n",
    "\n",
    "    kmeansp = MiniBatchKMeans(n_clusters=clusters, random_state=10).fit(X_new)\n",
    "    print('MiniBatchKMeans NMI: ', normalized_mutual_info_score(kmeansp.labels_, target_new))\n",
    "    print('MiniBatchKMeans AMI: ', adjusted_mutual_info_score(kmeansp.labels_, target_new))\n",
    "    print('MiniBatchKMeans Silhouette: ', silhouette_score(X_new, kmeansp.labels_, random_state=10))\n",
    "\n",
    "\n",
    "def similarity(X_new):\n",
    "    # Computing similarity\n",
    "    S = sparse.csr_matrix(cosine_similarity(X_new))\n",
    "    return S\n",
    "\n",
    "\n",
    "def RANMF(clusters, X_new, target_new, S):\n",
    "    #### The Proposed RANMF\n",
    "    Lambeda = 0.05\n",
    "    sumS = np.squeeze(np.array(S.sum(axis=1)))\n",
    "    D = random_state.rand(X_new.shape[0], clusters)\n",
    "    W = random_state.rand(clusters, X_new.shape[1])\n",
    "    for ite in range(200):\n",
    "        # print('Iteration: ', ite)\n",
    "        DW = D @ W\n",
    "        SD = S @ D\n",
    "\n",
    "        DW[DW <= .000001] = .000001\n",
    "        XDW = X_new / DW\n",
    "        XDWW = XDW @ W.transpose()\n",
    "        sumW = W.sum(axis=1)\n",
    "        M = []\n",
    "        for f in range(clusters):\n",
    "            M.append(sumW[f] + (2 * Lambeda * D[:, f] * sumS))\n",
    "        M = np.squeeze(M).transpose()\n",
    "        M[M == 0] = .000001\n",
    "        D = D * np.asarray(XDWW + (2 * Lambeda * SD)) / M\n",
    "        ####################\n",
    "        DW = D @ W\n",
    "        DW[DW <= .000001] = .000001\n",
    "        XDW = X_new / DW\n",
    "        XDWD = D.transpose() @ XDW\n",
    "        sumD = D.sum(axis=0)\n",
    "        W = W * np.asarray(XDWD)\n",
    "        T = []\n",
    "        for f in range(clusters):\n",
    "            if sumD[f] != 0:\n",
    "                T.append(W[f, :] / sumD[f])\n",
    "        T = np.squeeze(T)\n",
    "        W = T\n",
    "\n",
    "    RANMF_labels = np.argmax(D, axis=1)\n",
    "    print('RANMF NMI: ', normalized_mutual_info_score(RANMF_labels, target_new))\n",
    "    print('RANMF AMI: ', adjusted_mutual_info_score(RANMF_labels, target_new))\n",
    "    print('RANMF Silhouette: ', silhouette_score(X_new, RANMF_labels, random_state=10))\n",
    "\n",
    "\n",
    "def RANMF_converge(clusters, X_new, target_new, S):\n",
    "    #### The Proposed RANMF\n",
    "    Lambeda = 0.05\n",
    "    converge = []\n",
    "    before = 0\n",
    "    X_new2 = X_new\n",
    "    X_new2[X_new2 == 0] = .000001\n",
    "    X_new2 = np.array(X_new2)\n",
    "    sumS = np.squeeze(np.array(S.sum(axis=1)))\n",
    "    D = random_state.rand(X_new.shape[0], clusters)\n",
    "    W = random_state.rand(clusters, X_new.shape[1])\n",
    "    for ite in range(200):\n",
    "        # print('Iteration: ', ite)\n",
    "        DW = D @ W\n",
    "        SD = S @ D\n",
    "\n",
    "        DW[DW <= .000001] = .000001\n",
    "        XDW = X_new / DW\n",
    "        XDWW = XDW @ W.transpose()\n",
    "        sumW = W.sum(axis=1)\n",
    "        M = []\n",
    "        for f in range(clusters):\n",
    "            M.append(sumW[f] + (2 * Lambeda * D[:, f] * sumS))\n",
    "        M = np.squeeze(M).transpose()\n",
    "        M[M == 0] = .000001\n",
    "        D = D * np.asarray(XDWW + (2 * Lambeda * SD)) / M\n",
    "        ####################\n",
    "        DW = D @ W\n",
    "        DW[DW <= .000001] = .000001\n",
    "        XDW = X_new / DW\n",
    "        XDWD = D.transpose() @ XDW\n",
    "        sumD = D.sum(axis=0)\n",
    "        W = W * np.asarray(XDWD)\n",
    "        T = []\n",
    "        for f in range(clusters):\n",
    "            if sumD[f] != 0:\n",
    "                T.append(W[f, :] / sumD[f])\n",
    "        T = np.squeeze(T)\n",
    "        W = T\n",
    "\n",
    "        # # Convergence\n",
    "        DW[DW < .000001] = .000001\n",
    "        cost = (X_new2 * np.array(np.log(X_new2 / DW)) - X_new2 + DW).sum()\n",
    "        # print('Iteration: ', ite)\n",
    "        # cost = cost + Lambeda * ((euclidean_distances(D, D) ** 2) * np.array(S.todense())).sum()\n",
    "\n",
    "        # if (before - cost) < 0:\n",
    "        # print(' Not converge: ', ite)\n",
    "        before = cost\n",
    "        print(cost)\n",
    "        converge.append(cost)\n",
    "\n",
    "\n",
    "clusters = n_clusters\n",
    "\n",
    "\n",
    "# # With feature selection\n",
    "# X_new = feature_selection(X, features_number)\n",
    "\n",
    "# # Without feature selection\n",
    "# X_new = X\n",
    "\n",
    "# X_new, target_new = removing_zero_rows(X_new, target)\n",
    "X_new = x\n",
    "target_new = y\n",
    "compared_methods(clusters, X_new, target_new)\n",
    "\n",
    "S = similarity(X_new)\n",
    "\n",
    "RANMF(clusters, X_new, target_new, S)\n",
    "\n",
    "# RANMF_converge(clusters, X_new, target_new, S)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HVmWaaEucCii"
   },
   "source": [
    "## **Autoencoder and Encoder**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V2yKS3Df5U7N"
   },
   "outputs": [],
   "source": [
    "def autoencoder(dims, act='relu', init='glorot_uniform'):\n",
    "    \"\"\"\n",
    "    Fully connected auto-encoder model, symmetric.\n",
    "    Arguments:\n",
    "        dims: list of number of units in each layer of encoder. dims[0] is input dim, dims[-1] is units in hidden layer.\n",
    "            The decoder is symmetric with encoder. So number of layers of the auto-encoder is 2*len(dims)-1\n",
    "        act: activation, not applied to Input, Hidden and Output layers\n",
    "    return:\n",
    "        (ae_model, encoder_model), Model of autoencoder and model of encoder\n",
    "    \"\"\"\n",
    "    n_stacks = len(dims) - 1\n",
    "    # input\n",
    "    input_img = Input(shape=(dims[0],), name='input')\n",
    "    x = input_img\n",
    "    # internal layers in encoder\n",
    "    for i in range(n_stacks-1):\n",
    "        x = Dense(dims[i + 1], activation=act, kernel_initializer=init, name='encoder_%d' % i)(x)\n",
    "\n",
    "    # hidden layer\n",
    "    encoded = Dense(dims[-1], kernel_initializer=init, name='encoder_%d' % (n_stacks - 1))(x)  # hidden layer, features are extracted from here\n",
    "\n",
    "    x = encoded\n",
    "    # internal layers in decoder\n",
    "    for i in range(n_stacks-1, 0, -1):\n",
    "        x = Dense(dims[i], activation=act, kernel_initializer=init, name='decoder_%d' % i)(x)\n",
    "\n",
    "    # output\n",
    "    x = Dense(dims[0], kernel_initializer=init, name='decoder_0')(x)\n",
    "    decoded = x\n",
    "    return Model(inputs=input_img, outputs=decoded, name='AE'), Model(inputs=input_img, outputs=encoded, name='encoder')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yVsvQayR5U7h"
   },
   "source": [
    "## Hyper-params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S8OwWZ1o5U7h"
   },
   "outputs": [],
   "source": [
    "dims = [x.shape[-1], 500, 500, 2000, 10]\n",
    "init = VarianceScaling(scale=1. / 3., mode='fan_in', distribution='uniform', seed=10)\n",
    "pretrain_optimizer = SGD(learning_rate=1, momentum=0.9)\n",
    "pretrain_epochs = 500\n",
    "batch_size = 64\n",
    "save_dir = 'clustering'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HVg5gsE35U7j"
   },
   "outputs": [],
   "source": [
    "autoencoder, encoder = autoencoder(dims, init=init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibXdK2xA5U7m"
   },
   "outputs": [],
   "source": [
    "# plot_model(autoencoder, to_file='autoencoder.png', show_shapes=True)\n",
    "# Image(filename='autoencoder.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5_SNi1y65U7p"
   },
   "outputs": [],
   "source": [
    "# plot_model(encoder, to_file='encoder.png', show_shapes=True)\n",
    "# Image(filename='encoder.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4YpEvoEe5U7s"
   },
   "source": [
    "## Pretrain auto-encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cQLb7xRp5U7t",
    "outputId": "de558bec-fdb4-4f23-e949-57a29055e140",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "autoencoder.compile(optimizer=pretrain_optimizer, loss='mse')\n",
    "autoencoder.fit(x, x, batch_size=256, epochs=pretrain_epochs - 300) #, callbacks=cb)\n",
    "# # autoencoder.save_weights('/content/clustering/MyDrive/clustering/ae_augmentation_weights_20NEWSGROUPS.h5')\n",
    "# # autoencoder.save_weights('/content/clustering/MyDrive/clustering/ae_augmentation_weights_20NEWSGROUPS.h5')\n",
    "autoencoder.save_weights('clustering/ae_augmentation_weights_WEBKB.h5')\n",
    "#autoencoder.save_weights('clustering/ae_augmentation_weights_WEBKB_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H-SJ246u5U7x"
   },
   "source": [
    "### Load the pre-trained auto encoder weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gpCn0JN45U7x"
   },
   "outputs": [],
   "source": [
    "# autoencoder.load_weights('/content/clustering/MyDrive/clustering/ae_augmentation_weights_20NEWSGROUPS.h5')\n",
    "# autoencoder.load_weights('/content/clustering/MyDrive/clustering/ae_augmentation_weights_REUTERSIDF10K.h5')\n",
    "autoencoder.load_weights('clustering/ae_augmentation_weights_WEBKB.h5')\n",
    "#autoencoder.load_weights('clustering/ae_augmentation_weights_WEBKB_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U2GP-zLc5U70"
   },
   "source": [
    "## Build clustering model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "drBo2UY55U70"
   },
   "source": [
    "\n",
    "### Clusteringand embedding layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MOthAOw91DMC"
   },
   "outputs": [],
   "source": [
    "class ClusteringLayer(Layer):\n",
    "    \"\"\"\n",
    "    Clustering layer converts input sample (feature) to soft label, i.e. a vector that represents the probability of the\n",
    "    sample belonging to each cluster. The probability is calculated with student's t-distribution.\n",
    "\n",
    "    # Example\n",
    "    ```\n",
    "        model.add(ClusteringLayer(n_clusters=10))\n",
    "    ```\n",
    "    # Arguments\n",
    "        n_clusters: number of clusters.\n",
    "        weights: list of Numpy array with shape `(n_clusters, n_features)` witch represents the initial cluster centers.\n",
    "        alpha: degrees of freedom parameter in Student's t-distribution. Default to 1.0.\n",
    "    # Input shape\n",
    "        2D tensor with shape: `(n_samples, n_features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(n_samples, n_clusters)`.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, n_clusters, weights=None, alpha=1.0, **kwargs):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(ClusteringLayer, self).__init__(**kwargs)\n",
    "        self.n_clusters = n_clusters\n",
    "        self.alpha = alpha\n",
    "        self.initial_weights = weights\n",
    "        self.input_spec = InputSpec(ndim=2)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 2\n",
    "        input_dim = input_shape[1]\n",
    "        self.input_spec = InputSpec(dtype=K.floatx(), shape=(None, input_dim))\n",
    "        self.clusters = self.add_weight(shape=(self.n_clusters, input_dim), initializer=glorot_uniform(seed=10), name='clusters')\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "        self.built = True\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        \"\"\" student t-distribution, as same as used in t-SNE algorithm.\n",
    "         Measure the similarity between embedded point z_i and centroid µ_j.\n",
    "                 q_ij = 1/(1+dist(x_i, µ_j)^2), then normalize it.\n",
    "                 q_ij can be interpreted as the probability of assigning sample i to cluster j.\n",
    "                 (i.e., a soft assignment)\n",
    "        Arguments:\n",
    "            inputs: the variable containing data, shape=(n_samples, n_features)\n",
    "        Return:\n",
    "            q: student's t-distribution, or soft labels for each sample. shape=(n_samples, n_clusters)\n",
    "        \"\"\"\n",
    "\n",
    "        q = 1.0 / (1.0 + (K.sum(K.square(K.expand_dims(inputs, axis=1) - self.clusters), axis=2) / self.alpha))\n",
    "        q **= (self.alpha + 1.0) / 2.0\n",
    "        q = K.transpose(K.transpose(q) / K.sum(q, axis=1)) # Make sure each sample's 10 values add up to 1.\n",
    "        return q\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        assert input_shape and len(input_shape) == 2\n",
    "        return input_shape[0], self.n_clusters\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'n_clusters': self.n_clusters}\n",
    "        base_config = super(ClusteringLayer, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HyY5him5U72"
   },
   "outputs": [],
   "source": [
    "clustering_layer = ClusteringLayer(n_clusters, name='clustering')(encoder.output)\n",
    "Clustering_Loss_layer = tf.keras.layers.Concatenate(axis=1, name='Clustering_Loss')([clustering_layer, encoder.output])\n",
    "Recunstruction_Loss_layer  = tf.keras.layers.Concatenate(axis=1, name='Recunstruction_Loss')([autoencoder.output, encoder.output])\n",
    "\n",
    "Clustering_Loss_layer.trainable=False\n",
    "Recunstruction_Loss_layer.trainable=False\n",
    "\n",
    "model = Model(inputs=encoder.input, outputs=[Clustering_Loss_layer, Recunstruction_Loss_layer])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LCRzGuPZX9hi"
   },
   "outputs": [],
   "source": [
    "layer_names = [layer.name for layer in model.layers]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "tUUDtKHZQhtL",
    "outputId": "9af18bfd-361e-4ced-e879-7f4a9fa9f7d0"
   },
   "outputs": [],
   "source": [
    "print(layer_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rAsXWsD85U75"
   },
   "outputs": [],
   "source": [
    "# plot_model(model, to_file='model.png', show_shapes=True)\n",
    "# Image(filename='model.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8n2Gxnef5U7_"
   },
   "source": [
    "### Step 1: initialize cluster centers using k-means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NfZ_scLV5U7_",
    "outputId": "f242da90-bfde-4359-bdb1-bfed03c03798"
   },
   "outputs": [],
   "source": [
    "kmeans = KMeans(n_clusters=n_clusters, n_init=250, random_state=10)\n",
    "y_pred = kmeans.fit_predict(encoder.predict(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WthUCUuQ5U8B"
   },
   "outputs": [],
   "source": [
    "y_pred_last = np.copy(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QI570tUe3ivJ"
   },
   "outputs": [],
   "source": [
    "model.get_layer(name='clustering').set_weights([kmeans.cluster_centers_])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GCLoKb2P5U8F"
   },
   "source": [
    "### Step 2: deep clustering\n",
    "Compute p_i by first raising q_i to the second power and then normalizing by frequency per cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ysIy5qkx5U8G"
   },
   "outputs": [],
   "source": [
    "# computing an auxiliary target distribution\n",
    "def target_distribution(q):\n",
    "    weight = q ** 2 / q.sum(0)\n",
    "    return (weight.T / weight.sum(1)).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AqTmSvnt5U8I"
   },
   "outputs": [],
   "source": [
    "loss = 0\n",
    "index = 0\n",
    "#maxiter = 8000\n",
    "maxiter = 1000\n",
    "update_interval = 3\n",
    "index_array = np.arange(x.shape[0])\n",
    "tol = 0.001 # tolerance threshold to stop training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ydVVsski5U8N"
   },
   "source": [
    "### Start training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pW6G2ClPeC6Z"
   },
   "outputs": [],
   "source": [
    "def Clustering_Loss(y_true, y_pred):\n",
    "\n",
    "    kld_Loss = tf.keras.losses.kld(y_true[:,:n_clusters], y_pred[:,:n_clusters])\n",
    "\n",
    "    # calculate pairwise distance matrix\n",
    "    # pairwise_diff = K.expand_dims(y_pred[:,n_clusters:], 0) - K.expand_dims(y_pred[:,n_clusters:], 1)\n",
    "    # pairwise_squared_distance = K.sum(K.square(pairwise_diff), axis=-1)\n",
    "    # pairwise_distance = K.sqrt(pairwise_squared_distance + K.epsilon())\n",
    "\n",
    "    # calculate Cons distance matrix\n",
    "    # Cons_Simi = similarityMatrix.Calculate_Similarity_Matrix(4,x[idx])\n",
    "\n",
    "    return (1/len(idx) * K.sum(kld_Loss))\n",
    "    #return (1/len(idx) * K.sum(kld_Loss)) + 0.0001 * 1/len(idx) * K.sum(pairwise_distance * Cons_Simi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d0P_ALSMaDwI",
    "outputId": "c3331fdb-da8d-447b-83e0-73dcb5b8d8ca"
   },
   "outputs": [],
   "source": [
    "Z = encoder.output\n",
    "Z"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oDvxGnbpNEGq"
   },
   "outputs": [],
   "source": [
    "def Manifold_Loss():\n",
    "  this_batch_size = len(idx)\n",
    "  \n",
    "  this_batch_laplacian_matrix = np.zeros((this_batch_size,this_batch_size))\n",
    "  \n",
    "  for i in range(0,len(idx)):\n",
    "    for j in range(0,len(idx)):\n",
    "      this_batch_laplacian_matrix[i][j] = laplacian_matrix[idx[i]][idx[j]]\n",
    "\n",
    "  this_batch_inputs = np.asarray([main_inputs[i] for i in idx])\n",
    "  #print(\"this_batch_inputs: \", this_batch_inputs,\"\\n shape: \", this_batch_inputs.shape, \"\\n type: \", type(this_batch_inputs))\n",
    "\n",
    "\n",
    "  this_batch_laplacian_matrix = tf.cast(this_batch_laplacian_matrix, tf.float32)\n",
    "\n",
    "  lambda_hyperparameter = 100 # 100\n",
    "\n",
    "  #extract Z(latent) to compute Manifold_Loss\n",
    "  #Z = encoder.output\n",
    "\n",
    "  #convert keras tensor to numpy array\n",
    "  inp = encoder.input\n",
    "  output = encoder.output\n",
    "  functor = K.function([inp], [output])\n",
    "  Z = functor([this_batch_inputs])[0]\n",
    "\n",
    "  #print(\"type(Z):\", type(Z))\n",
    "  #print(\"-----------\")\n",
    "  #print(Z)\n",
    "\n",
    "  #Z_Transpose\n",
    "  Z_Transpose = np.transpose(Z)\n",
    "  \n",
    "  #print for debugging purposes\n",
    "  #print(\"type(Z_Transpose): \", type(Z_Transpose), \" Shape: \", Z_Transpose.shape)\n",
    "  #print(\"type(Z): \",type(Z), \" Shape: \", Z.shape)\n",
    "  #print(\"type(this_batch_laplacian_matrix): \", type(this_batch_laplacian_matrix), \" Shape: \", this_batch_laplacian_matrix.shape)\n",
    "  \n",
    "\n",
    "  temp = tf.tensordot(Z_Transpose, this_batch_laplacian_matrix, axes=1)\n",
    "  #temp = K.dot(Z_Transpose, this_batch_laplacian_matrix)\n",
    "  temp = tf.tensordot(temp, Z, axes=1)\n",
    "  #temp = K.dot(temp, Z)\n",
    "\n",
    "  #print(\"final_matrix: \", temp, \"\\n shape: \", temp.shape, \"\\n type: \", type(temp))\n",
    "  \n",
    "  #computing trace\n",
    "  #main_diagonal = [temp[i][i] for i in range(0,temp.shape[0])]\n",
    "  #print(\"main_diagonal: \", main_diagonal)\n",
    "  #temp_trace = K.sum(main_diagonal)\n",
    "\n",
    "  temp_trace = np.trace(temp)\n",
    "  \n",
    "  #print(\"trace: \", temp_trace)\n",
    "\n",
    "  manifold_loss = lambda_hyperparameter * temp_trace\n",
    "  #print(\"manifold_loss: \", manifold_loss)\n",
    "\n",
    "  return manifold_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t2ggKLhoC0h7"
   },
   "outputs": [],
   "source": [
    "def elastic_loss(batch_indexes, delta=0): # 0.001\n",
    "  \n",
    "  batch = x[batch_indexes]\n",
    "  #convert keras tensor to numpy array\n",
    "  inp = autoencoder.input\n",
    "  output = autoencoder.output\n",
    "  functor = K.function([inp], [output])\n",
    "  batch_output = functor([batch])[0]\n",
    "\n",
    "  l1_norm = np.sum(np.abs(batch - batch_output), axis=1)\n",
    "\n",
    "  l2_pseudo_loss = np.sum((delta * l1_norm ** 2)/(delta + l1_norm))\n",
    "  l1_pseudo_loss = np.sum((l1_norm ** 2)/(delta + l1_norm))\n",
    "\n",
    "  e_loss = l2_pseudo_loss + l1_pseudo_loss  \n",
    "    \n",
    "  # print(f\"elastic loss: {e_loss}\")\n",
    "\n",
    "  return e_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "SNRQDlJZC0h8"
   },
   "outputs": [],
   "source": [
    "def sparsity_loss(gamma=0.005): # 0.005\n",
    "  this_batch_size = len(idx)\n",
    "  \n",
    "  this_batch_laplacian_matrix = np.zeros((this_batch_size,this_batch_size))\n",
    "  \n",
    "  for i in range(0,len(idx)):\n",
    "    for j in range(0,len(idx)):\n",
    "      this_batch_laplacian_matrix[i][j] = laplacian_matrix[idx[i]][idx[j]]\n",
    "\n",
    "  this_batch_inputs = np.asarray([main_inputs[i] for i in idx])\n",
    "\n",
    "  this_batch_laplacian_matrix = tf.cast(this_batch_laplacian_matrix, tf.float32)\n",
    "\n",
    "  #convert keras tensor to numpy array\n",
    "  inp = encoder.input\n",
    "  output = encoder.output\n",
    "  functor = K.function([inp], [output])\n",
    "  Z = functor([this_batch_inputs])[0]\n",
    "\n",
    "  Z_transpose_norm = np.sum(np.square(np.sum(np.abs(Z), axis=0)))\n",
    "  \n",
    "  s_loss = gamma * Z_transpose_norm\n",
    "  # print(\"sparsity_loss: \", s_loss)\n",
    "\n",
    "  return s_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jpmB4QsZMFiy"
   },
   "outputs": [],
   "source": [
    "def total_loss(y_true, y_pred):\n",
    "    t_loss = elastic_loss(idx) + Manifold_Loss() + sparsity_loss()\n",
    "    print(f\"total loss: {t_loss}\")\n",
    "    return t_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QuCTVzct6ftt"
   },
   "outputs": [],
   "source": [
    "model.compile(loss=[Clustering_Loss, total_loss], loss_weights=[0.1, 1], optimizer=pretrain_optimizer, run_eagerly=True)\n",
    "#model.compile(loss=[Clustering_Loss, total_loss], loss_weights=[0.1, 1], optimizer=pretrain_optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2IzMeaUX5U8N",
    "outputId": "3a6e7063-f84e-491c-b43f-f8cf61af7ce8"
   },
   "outputs": [],
   "source": [
    "metrics_list = []\n",
    "max_ami = 0\n",
    "for ite in range(int(maxiter)):\n",
    "    if ite==1:\n",
    "      # model.load_weights(save_dir + '/Berahmand_text_20NG_n_cluster_12.h5')\n",
    "      # model.load_weights(save_dir + '/Berahmand_text_REUTERSIDF10K_n_cluster_6.h5')\n",
    "      # model.load_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4.h5')\n",
    "      model.load_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_500.h5')\n",
    "      \n",
    "\n",
    "\n",
    "    if ite % update_interval == 0:\n",
    "\n",
    "        q, _  = model.predict(x, verbose=0)\n",
    "        p = target_distribution(q[:,:n_clusters])  # update the auxiliary target distribution p\n",
    "\n",
    "        # evaluate the clustering performance\n",
    "        y_pred = q[:,:n_clusters].argmax(1)\n",
    "        if y is not None:\n",
    "            acc = np.round(acc_fun(y, y_pred), 5)\n",
    "            nmi = np.round(nmi_fun(y, y_pred), 5)\n",
    "            ari = np.round(ari_fun(y, y_pred), 5)\n",
    "            ami = np.round(ami_fun(y, y_pred), 5)\n",
    "            silhouette = np.round(silhouette_score_fun(x, y_pred, random_state=10), 5)\n",
    "             \n",
    "            loss = np.round(loss, 5)\n",
    "            print('Iter %d: acc = %.5f, nmi = %.5f, ari = %.5f, ami = %.5f, sc = %.5f' % (ite, acc, nmi, ari, ami, silhouette), ' ; loss=', loss)\n",
    "            metrics_list.append([ite, acc, nmi, ari, ami, silhouette])\n",
    "\n",
    "        # check stop criterion\n",
    "        delta_label = np.sum(y_pred != y_pred_last).astype(np.float32) / y_pred.shape[0]\n",
    "        y_pred_last = np.copy(y_pred)\n",
    "        if ite > 0 and delta_label < tol and acc>=1:\n",
    "            print('delta_label ', delta_label, '< tol ', tol)\n",
    "            print('Reached tolerance threshold. Stopping training.')\n",
    "            break\n",
    "        \n",
    "        if ami >= max_ami:\n",
    "          # model.save_weights(save_dir + '/Berahmand_text_20NG_n_cluster_12_best.h5')\n",
    "          # model.save_weights(save_dir + '/Berahmand_text_REUTERSIDF10K_n_cluster_6_best.h5')\n",
    "          # model.save_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_best.h5')\n",
    "          model.save_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_best_500.h5')\n",
    "          max_ami = ami\n",
    "\n",
    "    global idx\n",
    "    idx = index_array[index * batch_size: min((index+1) * batch_size, x.shape[0])]\n",
    "    #print(\"iter: \",ite,\"len(idx) not loss: \",len(idx),\", idx: \",idx)\n",
    "    loss = model.train_on_batch(x=x[idx], y=[np.concatenate((p[idx], np.ones((len(idx),dims[-1]))), axis=1), np.concatenate((x[idx], np.ones((len(idx),dims[-1]))), axis=1)])\n",
    "    index = index + 1 if (index + 1) * batch_size <= x.shape[0] else 0\n",
    "    # model.save_weights(save_dir + '/Berahmand_text_20NG_n_cluster_12.h5')\n",
    "    # model.save_weights(save_dir + '/Berahmand_text_REUTERSIDF10K_n_cluster_6.h5')\n",
    "    # model.save_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4.h5')\n",
    "    model.save_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ixj3uGD95U8P"
   },
   "source": [
    "### Load the clustering model trained weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2M1y68Q-5U8P"
   },
   "outputs": [],
   "source": [
    "# model.load_weights(save_dir + '/Berahmand_text_20NG_n_cluster_12_best.h5')\n",
    "# model.load_weights(save_dir + '/Berahmand_text_REUTERSIDF10K_n_cluster_6_best.h5')\n",
    "# model.load_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_best.h5')\n",
    "model.load_weights(save_dir + '/Berahmand_text_WEBKB_n_cluster_4_best_500.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RBgZ4ayY5U-d"
   },
   "source": [
    "### Final Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6IlU2s7B5U-f",
    "outputId": "d8db266d-7a38-44e1-f703-fc181c64cc2c"
   },
   "outputs": [],
   "source": [
    "# Eval.\n",
    "q,_ = model.predict(x, verbose=0)\n",
    "p = target_distribution(q[:,:n_clusters])  # update the auxiliary target distribution p\n",
    "\n",
    "# evaluate the clustering performance\n",
    "y_pred = q[:,:n_clusters].argmax(1)\n",
    "if y is not None:\n",
    "    acc = np.round(acc_fun(y, y_pred), 5)\n",
    "    nmi = np.round(nmi_fun(y, y_pred), 5)\n",
    "    ari = np.round(ari_fun(y, y_pred), 5)\n",
    "    ami = np.round(ami_fun(y, y_pred), 5)\n",
    "    silhouette = np.round(silhouette_score_fun(x, y_pred, random_state=10), 5)\n",
    "      \n",
    "    loss = np.round(loss, 5)\n",
    "    print('acc = %.5f, nmi = %.5f, ari = %.5f, ami = %.5f, , silhouette_score = %.5f' % (acc, nmi, ari, ami, silhouette), ' ; loss=', loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_list = np.array(metrics_list)\n",
    "plt.plot(metrics_list[:, 0], metrics_list[:, 2], label='nmi')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 891
    },
    "id": "H8cCYcyi5U-h",
    "outputId": "5be58abe-7106-4f2c-d3cb-632976d1039e"
   },
   "outputs": [],
   "source": [
    "sns.set(font_scale=3)\n",
    "\n",
    "plt.figure(figsize=(16, 14))\n",
    "sns.heatmap(confusion_matrix(y, y_pred), annot=True, fmt=\"d\", annot_kws={\"size\": 20});\n",
    "plt.title(\"Confusion matrix\", fontsize=30)\n",
    "plt.ylabel('True label', fontsize=25)\n",
    "plt.xlabel('Clustering label', fontsize=25)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8T0jR63_6Lot",
    "outputId": "5d9942f9-37e7-4900-b11d-8021541fbb11"
   },
   "outputs": [],
   "source": [
    "extract = Model(inputs = model.input, outputs = model.get_layer(layer_names[len(dims)-1]).output)\n",
    "z = extract.predict([x])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 575
    },
    "id": "yPIokghU2enm",
    "outputId": "fe00b9fc-114f-4cea-9a4e-b90ba3353669"
   },
   "outputs": [],
   "source": [
    "sns.set(context=\"paper\", style=\"white\")\n",
    "\n",
    "reducer = umap.UMAP(random_state=42)\n",
    "embedding = reducer.fit_transform(z)\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 10))\n",
    "color = y.astype(int)\n",
    "plt.scatter(embedding[:, 0], embedding[:, 1], c=color, cmap=\"Spectral\", s=10)\n",
    "plt.setp(ax, xticks=[], yticks=[])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
